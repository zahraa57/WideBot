{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcaffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os\n",
    "from collections import Counter\n",
    "from ar_wordcloud import ArabicWordCloud\n",
    "from bidi.algorithm import get_display\n",
    "import arabic_reshaper\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "import re, html, unicodedata ,string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c166ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re , emoji, Stemmer, functools, operator, string\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"allstories.csv\")\n",
    "df=df[[\"topic\",\"story\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9a0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=df.sort_values(by=\"topic\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe4d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = s.groupby('topic')\n",
    "test_data=grouped.tail(int(1000 * 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the test data from the original DataFrame to get the training data\n",
    "train_data = df.drop(test_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6353bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the training and test data\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a1be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarabic\n",
    "#!pip install emoji\n",
    "#!pip install pystemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c5abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "st =  Stemmer.Stemmer('arabic')\n",
    "def data_cleaning (text):\n",
    "  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "  text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "  text = re.sub(r\"http\\S+\", \"\", text)\n",
    "  text = re.sub(r\"https\\S+\", \"\", text)\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  text = re.sub(\"(\\s\\d+)\",\"\",text) \n",
    "  text = re.sub(r\"$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$\", \"\", text)\n",
    "  text = re.sub(\"\\d+\", \" \", text)\n",
    "  #text = ar.strip_tashkeel(text)\n",
    "  #text = ar.strip_tatweel(text)\n",
    "  text = text.replace(\"#\", \" \");\n",
    "  text = text.replace(\"@\", \" \");\n",
    "  text = text.replace(\"_\", \" \");\n",
    "  translator = str.maketrans('', '', string.punctuation)\n",
    "  text = text.translate(translator)\n",
    "  #em = text\n",
    "  #em_split_emoji = emoji.get_emoji_regexp().split(em)\n",
    "  #em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "  #em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "  #text = \" \".join(em_split)\n",
    "  text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "  text_stem = \" \".join([st.stemWord(i) for i in text.split()])\n",
    "  text = text +\" \"+ text_stem\n",
    "  text = text.replace(\"آ\", \"ا\")\n",
    "  text = text.replace(\"إ\", \"ا\")\n",
    "  text = text.replace(\"أ\", \"ا\")\n",
    "  text = text.replace(\"ؤ\", \"و\")\n",
    "  text = text.replace(\"ئ\", \"ي\")\n",
    "   \n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"cleaned\"] = train_data[\"story\"].apply(lambda x:   data_cleaning(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e7b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"cleaned\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    " #First setting the max_len , will be useful later for BERT Model\n",
    "Extra_Len = 6 # an extra padding in length , found to be useful for increasing F-score\n",
    "Max_Len = train_data[\"cleaned\"].str.split().str.len().max() + Extra_Len\n",
    "print(Max_Len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_data['encodedLabel'] = label_encoder.fit_transform(train_data['topic'])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b28049",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_data['encodedLabel'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a183c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_WORDS = 11838\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_data[\"story\"])\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data[\"story\"])\n",
    "print(len(train_sequences))\n",
    "print(train_sequences[:3])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data[\"story\"])\n",
    "print(len(test_sequences))\n",
    "print(test_sequences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f1ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "train_padded = pad_sequences(train_sequences, padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "print(train_padded.shape)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "print(test_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.metrics import  Precision, Recall, Accuracy\n",
    "\n",
    "EMB_SIZE = 128\n",
    "\n",
    "model = Sequential([\n",
    "           Embedding(MAX_WORDS, EMB_SIZE, mask_zero=True, input_length=MAX_LEN),\n",
    "           LSTM(128),\n",
    "           Dense(11, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=[ Precision(), Recall(), Accuracy()]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46981b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b57c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_padded,\n",
    "                    train_labels,\n",
    "                    epochs=10,\n",
    "                    #batch_size=256,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b34b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d7f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
